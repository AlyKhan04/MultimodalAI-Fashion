{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444b9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alykhan/Downloads/MultimodalAIFashion/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "from pathlib import Path\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cceab701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths (Identical to Data_Exploration_Preprocessing.ipynb)\n",
    "RAW = Path('../data/raw/fashion-dataset')\n",
    "PROCESSED = Path('../data/processed')\n",
    "styles_csv = RAW / 'styles.csv'\n",
    "images_dir = RAW / 'images'\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not styles_csv.exists():\n",
    "    raise FileNotFoundError(f\"Run the download script first. Missing: {styles_csv}\")\n",
    "if not images_dir.exists():\n",
    "    raise FileNotFoundError(f\"Images folder not found at: {images_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd0ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 44424\n"
     ]
    }
   ],
   "source": [
    "#Load raw CSV\n",
    "df = pd.read_csv(styles_csv, on_bad_lines='skip')\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "sample_df = df.sample(n=16, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a033ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping missing/invalid images: 44419 rows\n",
      "Remaining dataset size: 44283\n",
      "Remaining number of classes: 109\n"
     ]
    }
   ],
   "source": [
    "#Drop rows with missing articleType\n",
    "df = df.dropna(subset=[\"articleType\"])\n",
    "\n",
    "#Keep only rows where the image file exists\n",
    "df[\"image_path\"] = df[\"id\"].apply(lambda x: images_dir / f\"{x}.jpg\")\n",
    "df = df[df[\"image_path\"].apply(lambda x: x.exists())].reset_index(drop=True)\n",
    "print(f\"After dropping missing/invalid images: {len(df)} rows\")\n",
    "\n",
    "#Remove rare classes\n",
    "MIN_SAMPLES = 9\n",
    "class_counts = df[\"articleType\"].value_counts()\n",
    "valid_classes = class_counts[class_counts >= MIN_SAMPLES].index\n",
    "df = df[df[\"articleType\"].isin(valid_classes)].reset_index(drop=True)\n",
    "print(f\"Remaining dataset size: {len(df)}\")\n",
    "print(f\"Remaining number of classes: {df['articleType'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa39bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations as planned in previous notebook\n",
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65269777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 109\n",
      "Final dataset size: 44283\n"
     ]
    }
   ],
   "source": [
    "#Label encoding + mapping - uses saved mappings from previous notebook/json if found\n",
    "label_mapping_path = PROCESSED / \"label_mapping.json\"\n",
    "if label_mapping_path.exists():\n",
    "    with open(label_mapping_path, \"r\") as f:\n",
    "        label_mapping = json.load(f)\n",
    "else:\n",
    "    #Builds mapping from current filtered df\n",
    "    classes = sorted(df[\"articleType\"].unique())\n",
    "    label_mapping = {cls: i for i, cls in enumerate(classes)}\n",
    "    with open(label_mapping_path, \"w\") as f:\n",
    "        json.dump(label_mapping, f, indent=2)\n",
    "    print(\"Saved:\", label_mapping_path)\n",
    "\n",
    "df[\"label_id\"] = df[\"articleType\"].map(label_mapping).astype(int)\n",
    "print(\"Number of classes:\", len(label_mapping))\n",
    "\n",
    "# Cleans text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_name\"] = df[\"productDisplayName\"].apply(clean_text)\n",
    "print(\"Final dataset size:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4b026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_transform=None, text_transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_transform = image_transform\n",
    "        self.text_transform = text_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "        text = row[\"clean_name\"]\n",
    "        if self.text_transform:\n",
    "            text = self.text_transform(text)\n",
    "        label = int(row[\"label_id\"])\n",
    "        return img, text, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc88224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Val/Test split (use saved split if present)\n",
    "split_indices_path = PROCESSED / \"split_indices.json\"\n",
    "if split_indices_path.exists():\n",
    "    with open(split_indices_path, \"r\") as f:\n",
    "        split_indices = json.load(f)\n",
    "    train_index = split_indices[\"train_index\"]\n",
    "    val_index   = split_indices[\"val_index\"]\n",
    "    test_index  = split_indices[\"test_index\"]\n",
    "else:\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    RANDOM_STATE = 42\n",
    "    split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=RANDOM_STATE)\n",
    "    train_index, temp_index = next(split1.split(df, df[\"label_id\"]))\n",
    "\n",
    "    df_train = df.iloc[train_index].reset_index(drop=True)\n",
    "    df_temp  = df.iloc[temp_index].reset_index(drop=True)\n",
    "\n",
    "    split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=RANDOM_STATE)\n",
    "    val_index_rel, test_index_rel = next(split2.split(df_temp, df_temp[\"label_id\"]))\n",
    "    # map relative indices back to original df indices\n",
    "    val_index  = np.array(temp_index)[val_index_rel].tolist()\n",
    "    test_index = np.array(temp_index)[test_index_rel].tolist()\n",
    "\n",
    "    split_indices = {\n",
    "        \"train_index\": list(map(int, train_index)),\n",
    "        \"val_index\":   list(map(int, val_index)),\n",
    "        \"test_index\":  list(map(int, test_index)),\n",
    "    }\n",
    "    with open(split_indices_path, \"w\") as f:\n",
    "        json.dump(split_indices, f, indent=2)\n",
    "    print(\"Saved split indices:\", split_indices_path)\n",
    "\n",
    "# Build split DataFrames\n",
    "train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "val_df   = df.iloc[val_index].reset_index(drop=True)\n",
    "test_df  = df.iloc[test_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a00bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "train_dataset = FashionDataset(train_df, image_transform=train_transformations)\n",
    "val_dataset   = FashionDataset(val_df,   image_transform=val_transforms)\n",
    "test_dataset  = FashionDataset(test_df,  image_transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec79279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class weights\n",
    "class_weights_path = PROCESSED / \"class_weights.pt\"\n",
    "if class_weights_path.exists():\n",
    "    class_weights = torch.load(class_weights_path, map_location=\"cpu\")\n",
    "else:\n",
    "    counts = Counter(df[\"label_id\"])\n",
    "    num_classes = len(counts)\n",
    "    total_samples = sum(counts.values())\n",
    "    class_weights = torch.tensor(\n",
    "        [total_samples / (num_classes * counts[i]) for i in range(num_classes)],\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    torch.save(class_weights, class_weights_path)\n",
    "    print(\"Saved class weights:\", class_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a11dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Step 1 ready ==\n",
      "Classes: 109\n",
      "Train/Val/Test sizes: 30998 / 6642 / 6643\n",
      "Batch: (32, 3, 224, 224) | Example text: jealous 21 women white flower top purple stripes top | Label: 96\n"
     ]
    }
   ],
   "source": [
    "#Sanity check batch\n",
    "imgs, texts, labels = next(iter(train_loader))\n",
    "print(\"== Step 1 ready ==\")\n",
    "print(f\"Classes: {len(label_mapping)}\")\n",
    "print(f\"Train/Val/Test sizes: {len(train_dataset)} / {len(val_dataset)} / {len(test_dataset)}\")\n",
    "print(\"Batch:\", tuple(imgs.shape), \"| Example text:\", texts[0][:60] if isinstance(texts[0], str) else texts[0], \"| Label:\", int(labels[0]))\n",
    "\n",
    "#Make variables global for later use\n",
    "globals().update(dict(\n",
    "    df=df, train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "    class_weights=class_weights, label_mapping=label_mapping,\n",
    "    train_transformations=train_transformations, val_transforms=val_transforms,\n",
    "    FashionDataset=FashionDataset\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe61c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Small utility: parameter count\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "#Basic convolutional block: Conv2d -> BN -> ReLU -> MaxPool\n",
    "class ConvBNReLU(nn.Module):\n",
    "    #inchannels: number of input channels\n",
    "    #outchannels: number of filters in block output\n",
    "    #pooling: True to downsample, False to not downsample\n",
    "    #p_drop: probability of dropout layers\n",
    "    def __init__(self, in_channels, out_channels, pooling=False, p_drop=0.0):\n",
    "        super().__init__()\n",
    "        #Adding the blocks\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(2) if pooling else nn.Identity()\n",
    "        self.drop = nn.Dropout2d(p_drop) if p_drop > 0 else nn.Identity()\n",
    "    \n",
    "    #Defines the computation for an input tensor x of shape (B, in_channels, H, W).\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class ImageEncoderCNN(nn.Module):\n",
    "    def __init__(self, feature_dim=256, in_channels=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBNReLU(in_channels,   32, pooling=True,  p_drop=0.05),  # 224 -> 112\n",
    "            ConvBNReLU(32,      64, pooling=True,  p_drop=0.05),  # 112 -> 56\n",
    "            ConvBNReLU(64,     128, pooling=True,  p_drop=0.05),  # 56  -> 28\n",
    "            ConvBNReLU(128,    256, pooling=True,  p_drop=0.05),  # 28  -> 14\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc  = nn.Linear(256, feature_dim)\n",
    "        self.bn  = nn.BatchNorm1d(feature_dim)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.gap(x).flatten(1)  # (B, 256)\n",
    "        x = self.fc(x)              # (B, feature_dim)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        return x  # feature vector\n",
    "\n",
    "class ImageClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes, feature_dim=256):\n",
    "        super().__init__()\n",
    "        #Initialise an encoder\n",
    "        self.encoder = ImageEncoderCNN(feature_dim=feature_dim)\n",
    "        self.head = nn.Linear(feature_dim, num_classes)\n",
    "        \n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        return self.head.in_features\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        z = self.encoder(x)            # (B, feature_dim)\n",
    "        logits = self.head(z)          # (B, num_classes)\n",
    "        if return_features:\n",
    "            return logits, z\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2cc8b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageEncoder params: 455,200\n",
      "Total model params:  483,213\n",
      "Logits shape: (32, 109)\n",
      "Features shape: (32, 256)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(label_mapping)\n",
    "model = ImageClassifierCNN(num_classes=num_classes, feature_dim=256).to(device)\n",
    "\n",
    "print(f\"ImageEncoder params: {count_params(model.encoder):,}\")\n",
    "print(f\"Total model params:  {count_params(model):,}\")\n",
    "\n",
    "# Dry forward on a mini-batch to verify shapes\n",
    "xb, _, yb = next(iter(train_loader))\n",
    "xb = xb.to(device)\n",
    "with torch.no_grad():\n",
    "    logits, feats = model(xb, return_features=True)\n",
    "print(\"Logits shape:\", tuple(logits.shape))   # (B, num_classes)\n",
    "print(\"Features shape:\", tuple(feats.shape))  # (B, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b527ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 20] loss=4.7692 acc=2.81%\n",
      "[batch 40] loss=4.7137 acc=4.30%\n",
      "[batch 60] loss=4.6832 acc=4.58%\n",
      "[batch 80] loss=4.6515 acc=5.47%\n",
      "[batch 100] loss=4.6185 acc=5.56%\n",
      "VAL: loss=4.5704 acc=4.56%\n"
     ]
    }
   ],
   "source": [
    "# ==== OPTIONAL: quick smoke test for the CNN (a few batches) ====\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device) if class_weights is not None else None)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "model.train()\n",
    "max_batches = 100  # keep it short; raise if you want\n",
    "running_loss, correct, seen = 0.0, 0, 0\n",
    "\n",
    "for b, (xb, _, yb) in enumerate(train_loader, 1):\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with autocast(enabled=torch.cuda.is_available()):\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    running_loss += loss.item() * xb.size(0)\n",
    "    preds = logits.argmax(dim=1)\n",
    "    correct += (preds == yb).sum().item()\n",
    "    seen += xb.size(0)\n",
    "\n",
    "    if b % 20 == 0 or b == max_batches:\n",
    "        print(f\"[batch {b}] loss={(running_loss/seen):.4f} acc={(correct/seen)*100:.2f}%\")\n",
    "\n",
    "    if b >= max_batches:\n",
    "        break\n",
    "\n",
    "# quick val pass\n",
    "model.eval()\n",
    "val_correct, val_seen, val_loss = 0, 0, 0.0\n",
    "with torch.no_grad():\n",
    "    for xb, _, yb in val_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        val_loss += criterion(logits, yb).item() * xb.size(0)\n",
    "        val_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
    "        val_seen += xb.size(0)\n",
    "\n",
    "print(f\"VAL: loss={(val_loss/val_seen):.4f} acc={(val_correct/val_seen)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7fc06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fashion-ai) finak",
   "language": "python",
   "name": "fashion-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
