{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444b9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alykhan/Downloads/MultimodalAIFashion/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "from pathlib import Path\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cceab701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths (Identical to Data_Exploration_Preprocessing.ipynb)\n",
    "RAW = Path('../data/raw/fashion-dataset')\n",
    "PROCESSED = Path('../data/processed')\n",
    "styles_csv = RAW / 'styles.csv'\n",
    "images_dir = RAW / 'images'\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not styles_csv.exists():\n",
    "    raise FileNotFoundError(f\"Run the download script first. Missing: {styles_csv}\")\n",
    "if not images_dir.exists():\n",
    "    raise FileNotFoundError(f\"Images folder not found at: {images_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd0ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 44424\n"
     ]
    }
   ],
   "source": [
    "#Load raw CSV\n",
    "df = pd.read_csv(styles_csv, on_bad_lines='skip')\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "sample_df = df.sample(n=16, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a033ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping missing/invalid images: 44419 rows\n",
      "Remaining dataset size: 44283\n",
      "Remaining number of classes: 109\n"
     ]
    }
   ],
   "source": [
    "#Drop rows with missing articleType\n",
    "df = df.dropna(subset=[\"articleType\"])\n",
    "\n",
    "#Keep only rows where the image file exists\n",
    "df[\"image_path\"] = df[\"id\"].apply(lambda x: images_dir / f\"{x}.jpg\")\n",
    "df = df[df[\"image_path\"].apply(lambda x: x.exists())].reset_index(drop=True)\n",
    "print(f\"After dropping missing/invalid images: {len(df)} rows\")\n",
    "\n",
    "#Remove rare classes\n",
    "MIN_SAMPLES = 9\n",
    "class_counts = df[\"articleType\"].value_counts()\n",
    "valid_classes = class_counts[class_counts >= MIN_SAMPLES].index\n",
    "df = df[df[\"articleType\"].isin(valid_classes)].reset_index(drop=True)\n",
    "print(f\"Remaining dataset size: {len(df)}\")\n",
    "print(f\"Remaining number of classes: {df['articleType'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa39bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations as planned in previous notebook\n",
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65269777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 109\n",
      "Final dataset size: 44283\n"
     ]
    }
   ],
   "source": [
    "#Label encoding + mapping - uses saved mappings from previous notebook/json if found\n",
    "label_mapping_path = PROCESSED / \"label_mapping.json\"\n",
    "if label_mapping_path.exists():\n",
    "    with open(label_mapping_path, \"r\") as f:\n",
    "        label_mapping = json.load(f)\n",
    "else:\n",
    "    #Builds mapping from current filtered df\n",
    "    classes = sorted(df[\"articleType\"].unique())\n",
    "    label_mapping = {cls: i for i, cls in enumerate(classes)}\n",
    "    with open(label_mapping_path, \"w\") as f:\n",
    "        json.dump(label_mapping, f, indent=2)\n",
    "    print(\"Saved:\", label_mapping_path)\n",
    "\n",
    "df[\"label_id\"] = df[\"articleType\"].map(label_mapping).astype(int)\n",
    "print(\"Number of classes:\", len(label_mapping))\n",
    "\n",
    "# Cleans text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_name\"] = df[\"productDisplayName\"].apply(clean_text)\n",
    "print(\"Final dataset size:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4b026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_transform=None, text_transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_transform = image_transform\n",
    "        self.text_transform = text_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "        text = row[\"clean_name\"]\n",
    "        if self.text_transform:\n",
    "            text = self.text_transform(text)\n",
    "        label = int(row[\"label_id\"])\n",
    "        return img, text, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc88224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Val/Test split (use saved split if present)\n",
    "split_indices_path = PROCESSED / \"split_indices.json\"\n",
    "if split_indices_path.exists():\n",
    "    with open(split_indices_path, \"r\") as f:\n",
    "        split_indices = json.load(f)\n",
    "    train_index = split_indices[\"train_index\"]\n",
    "    val_index   = split_indices[\"val_index\"]\n",
    "    test_index  = split_indices[\"test_index\"]\n",
    "else:\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    RANDOM_STATE = 42\n",
    "    split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=RANDOM_STATE)\n",
    "    train_index, temp_index = next(split1.split(df, df[\"label_id\"]))\n",
    "\n",
    "    df_train = df.iloc[train_index].reset_index(drop=True)\n",
    "    df_temp  = df.iloc[temp_index].reset_index(drop=True)\n",
    "\n",
    "    split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=RANDOM_STATE)\n",
    "    val_index_rel, test_index_rel = next(split2.split(df_temp, df_temp[\"label_id\"]))\n",
    "    # map relative indices back to original df indices\n",
    "    val_index  = np.array(temp_index)[val_index_rel].tolist()\n",
    "    test_index = np.array(temp_index)[test_index_rel].tolist()\n",
    "\n",
    "    split_indices = {\n",
    "        \"train_index\": list(map(int, train_index)),\n",
    "        \"val_index\":   list(map(int, val_index)),\n",
    "        \"test_index\":  list(map(int, test_index)),\n",
    "    }\n",
    "    with open(split_indices_path, \"w\") as f:\n",
    "        json.dump(split_indices, f, indent=2)\n",
    "    print(\"Saved split indices:\", split_indices_path)\n",
    "\n",
    "# Build split DataFrames\n",
    "train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "val_df   = df.iloc[val_index].reset_index(drop=True)\n",
    "test_df  = df.iloc[test_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a00bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "train_dataset = FashionDataset(train_df, image_transform=train_transformations)\n",
    "val_dataset   = FashionDataset(val_df,   image_transform=val_transforms)\n",
    "test_dataset  = FashionDataset(test_df,  image_transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec79279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class weights\n",
    "class_weights_path = PROCESSED / \"class_weights.pt\"\n",
    "if class_weights_path.exists():\n",
    "    class_weights = torch.load(class_weights_path, map_location=\"cpu\")\n",
    "else:\n",
    "    counts = Counter(df[\"label_id\"])\n",
    "    num_classes = len(counts)\n",
    "    total_samples = sum(counts.values())\n",
    "    class_weights = torch.tensor(\n",
    "        [total_samples / (num_classes * counts[i]) for i in range(num_classes)],\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    torch.save(class_weights, class_weights_path)\n",
    "    print(\"Saved class weights:\", class_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a11dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Step 1 ready ==\n",
      "Classes: 109\n",
      "Train/Val/Test sizes: 30998 / 6642 / 6643\n",
      "Batch: (32, 3, 224, 224) | Example text: marvel boys blue slippers | Label: 30\n"
     ]
    }
   ],
   "source": [
    "#Sanity check batch\n",
    "imgs, texts, labels = next(iter(train_loader))\n",
    "print(\"== Step 1 ready ==\")\n",
    "print(f\"Classes: {len(label_mapping)}\")\n",
    "print(f\"Train/Val/Test sizes: {len(train_dataset)} / {len(val_dataset)} / {len(test_dataset)}\")\n",
    "print(\"Batch:\", tuple(imgs.shape), \"| Example text:\", texts[0][:60] if isinstance(texts[0], str) else texts[0], \"| Label:\", int(labels[0]))\n",
    "\n",
    "#Make variables global for later use\n",
    "globals().update(dict(\n",
    "    df=df, train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "    class_weights=class_weights, label_mapping=label_mapping,\n",
    "    train_transformations=train_transformations, val_transforms=val_transforms,\n",
    "    FashionDataset=FashionDataset\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe61c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Small utility: parameter count\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "#Basic convolutional block: Conv2d -> BN -> ReLU -> MaxPool\n",
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, pool=False, p_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_ch)\n",
    "        self.act  = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(2) if pool else nn.Identity()\n",
    "        self.drop = nn.Dropout2d(p_drop) if p_drop > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "#Image encoder: produces a fixed-length feature vector\n",
    "class ImageEncoderCNN(nn.Module):\n",
    "    def __init__(self, feature_dim=256, in_ch=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 224x224 -> 112 -> 56 -> 28 -> 14\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBNReLU(in_ch,   32, pool=True,  p_drop=0.05),  # 224 -> 112\n",
    "            ConvBNReLU(32,      64, pool=True,  p_drop=0.05),  # 112 -> 56\n",
    "            ConvBNReLU(64,     128, pool=True,  p_drop=0.05),  # 56  -> 28\n",
    "            ConvBNReLU(128,    256, pool=True,  p_drop=0.05),  # 28  -> 14\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # (B, 256, 1, 1)\n",
    "        self.fc  = nn.Linear(256, feature_dim)\n",
    "        self.bn  = nn.BatchNorm1d(feature_dim)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.gap(x).flatten(1)  # (B, 256)\n",
    "        x = self.fc(x)              # (B, feature_dim)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        return x  # feature vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc8b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fashion-ai) finak",
   "language": "python",
   "name": "fashion-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
